Statistics in R
========================================================
author: MRC London Institute of Medical Sciences (http://bioinformatics.lms.mrc.ac.uk)
date: 25/Sep/2018
width: 1440
height: 1100
autosize: true
font-import: <link href='http://fonts.googleapis.com/css?family=Slabo+27px' rel='stylesheet' type='text/css'>
font-family: 'Slabo 27px', serif;
css:style.css

Outline
========================================================

- data summary

- hypothesis testing

- correlation, linear regression and ANOVA

- we only have 1 hour and 45 minutes...

Materials.
========================================================
id: materials

All prerequisites, links to material and slides for this course can be found on github.
* [StatisticsInR](http://mrccsc.github.io/StatisticsInR/)

Or can be downloaded as a zip archive from here.
* [Download zip](https://github.com/mrccsc/StatisticsInR/zipball/master)

Materials. - Presentations, source code and practicals.
========================================================

Once the zip files are unzipped, all presentations are available as HTML slides and pages. Practical sheets will be available in the directories underneath.

* **exercises/**
Practicals as HTML pages.
* **answers/**
Practicals with answers as HTML pages.

Set the Working directory
========================================================

Before running any of the code in the practicals or slides we need to set the working directory to the folder we unarchived.

You may navigate to the unarchived Reproducible-R folder in the Rstudio menu

**Session -> Set Working Directory -> Choose Directory**

or in the console.

```{r,eval=F}
setwd("/Users/yfwang27/workfromhome/workshop/StatisticsR_CBW18")
# e.g. setwd("~/Downloads/StatisticsInR/Statistics2016")
```

Case study
========================================================
Start from some data for mouse strain BKS.Cg-Dock7m +/+ Leprdb/J (db/db)

Data Source: Jackson Laboratory

<img src="figure/good_format.png";>

Save excel file as csv file
========================================================
<img src="figure/save_as_CSV.png";>

We don't need a pretty excel file for the csv format
========================================================
<img src="figure/pretty_format.png";>

Load data
========================================================

```{r,prompt=T}
alldata<-read.csv(file="data/mouse_BW_Fat_Glu_info.csv")
```

Data summary
========================================================

- data type

- spread of data

- shape of data

- distribution


Data type
========================================================

data type

- continuous

eg. blood pressure, body weight, height ...

- discrete

eg. gender, hair colour, RNA-seq read count...


Recap some basic R functions (1/7)
========================================================
- **str()**, **head()**, **dim()**, **colnames()** and **summary()** functions
- more details please see [Reproducible R course](http://mrccsc.github.io/Reproducible-R/)

-- [Data types in R](http://mrccsc.github.io/Reproducible-R/r_course/presentations/slides/introToR_Session1.html#/datatypes)


Recap some basic R functions (2/7)
========================================================

**head()**: See the first 6 lines of an R object
```{r,prompt=T}
head(alldata)
```

Recap some basic R functions (3/7)
========================================================
see the how many rows and columns in "alldata" object
```{r,prompt=T}
dim(alldata)
```

see column names of "alldata"
```{r,prompt=T}
colnames(alldata)
```

Recap some basic R functions (4/7)
========================================================

**str()**: Compactly display the internal structure of an R object

Make sure the data format is correct for each column.
```{r,prompt=T}
str(alldata)
```

Recap some basic R functions (5/7)
========================================================

```{r,prompt=T}
summary(alldata)
```

Recap some basic R functions (6/7)
========================================================

```{r,prompt=T}
alldata$Age<-as.factor(alldata$Age)
summary(alldata)
```

Recap some basic R functions (7/7)
========================================================

**ftable()**: Create ‘flat’ contingency tables
```{r}
ftable(alldata[,c("Age","Genotype")])
```

Spread of data - working with plots (boxplot)
========================================================
```{r,prompt=T}
library(ggplot2)
ggplot(alldata, aes(x=Genotype, y=BW.gram, fill=Age)) + geom_boxplot()
```

Shape of data - working with plots 2 (Violin plot)
========================================================
```{r,prompt=T}
library(ggplot2)
ggplot(alldata, aes(x=Genotype, y=BW.gram, fill=Age)) + geom_violin(position=position_dodge(width = 0.5)) +geom_boxplot(width=.1, outlier.colour=NA,position=position_dodge(width = 0.5))

```


Spread of data - use body weight from WT mice (1/4)
========================================================

- working with plots
- more details please see [Reproducible R course](http://mrccsc.github.io/Reproducible-R/)

--[Plotting in R](http://mrccsc.github.io/Reproducible-R/r_course/presentations/slides/introToR_Session1.html#/plotting)

```{r,prompt=T}
WT_data<-alldata[alldata$Genotype=="+/+",]
WT_data$BW.gram
```

Spread of data - useful functions (2/4)
========================================================

**min()**, **max()**, **median()**, **range()** and **quantile()** functions
```{r,prompt=T}
min.BW.gram<-min(WT_data$BW.gram)
max.BW.gram<-max(WT_data$BW.gram)
median.BW.gram<-median(WT_data$BW.gram)
mean.BW.gram<-mean(WT_data$BW.gram)
c(min.BW.gram, max.BW.gram, median.BW.gram, mean.BW.gram)
range(WT_data$BW.gram)
quantile(WT_data$BW.gram)
```

Spread of data - work with plots (3/4)
========================================================
Left: 40%

Boxplot

```{r, echo=FALSE,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
boxplot(WT_data$BW.gram,ylab="Body Weight (gram)")
abline(h=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
```
***
```{r,prompt=T}
c(min.BW.gram, max.BW.gram)
c(median.BW.gram, mean.BW.gram)
quantile(WT_data$BW.gram)[c(2,4)]
```

Spread of data -  work with plots (4/4)
========================================================
**summary()**
```{r,prompt=T}
summary(WT_data$BW.gram)
```
**range()**: show the minimum and maximum
```{r,prompt=T}
range(WT_data$BW.gram)
```
**IQR()**: show the interquartile range, i.e. 3rd quartile - 1st quartile
```{r,prompt=T}
IQR(WT_data$BW.gram)
```



Spread of data - more about boxplot (optional 1/3)
========================================================

Scatter plot: plot the WT mice's Body Weight against index

```{r,prompt=T,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
plot(WT_data$BW.gram,ylab="Body Weight (gram)")
```

Spread of data - work with plots (optional 2/3)
========================================================
sort the data from min to max

```{r,prompt=T,echo=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)")
```
***

start to see something here...

```{r,echo=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
quan_1<-quantile(c(1:nrow(WT_data)))[2]
quan_3<-quantile(c(1:nrow(WT_data)))[4]
plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)",xaxt="n")
  axis(side=1, at=c(0,quan_1,median(1:nrow(WT_data)),quan_3,nrow(WT_data)),
     labels=c("min","1stquantile","median","3rdquantile","max"))
  points(x=c(1,median(1:nrow(WT_data)),nrow(WT_data)),
         y=c(min(WT_data$BW.gram),
             median(WT_data$BW.gram),max(WT_data$BW.gram)),
         col="red",pch=16,cex=1.6)
  abline(h=quantile(WT_data$BW.gram)[c(2,4)],
         v=quantile(c(1:nrow(WT_data)))[c(2,4)],col="pink",lwd=3,lty=2)
```


Spread of data - work with plots (optional 3/3)
========================================================

```{r, echo=FALSE,fig.width=6,fig.height=6,dpi=300,out.width="750px",height="720px"}
par(mfrow=c(1,2))
  boxplot(WT_data$BW.gram,ylab="Body Weight (gram)")
  plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)",xaxt="n")
  axis(side=1, at=c(0,quan_1,median(1:nrow(WT_data)),quan_3,nrow(WT_data)),
     labels=c("min","1stquantile","median","3rdquantile","max"))
  points(x=c(1,median(1:nrow(WT_data)),nrow(WT_data)),
         y=c(min(WT_data$BW.gram),
             median(WT_data$BW.gram),max(WT_data$BW.gram)),
         col="red",pch=16,cex=1.6)
  abline(h=quantile(WT_data$BW.gram)[c(2,4)],
         v=quantile(c(1:nrow(WT_data)))[c(2,4)],col="pink",lwd=3,lty=2)
par(mfrow=c(1,1))
```

Data shape - histogram (1/4)
========================================================

```{r,prompt=T,fig.width=4.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
hist(WT_data$BW.gram,breaks=10)
```

Data shape - histogram (2/4)
========================================================

```{r,prompt=T,fig.width=4.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
hist(WT_data$BW.gram,breaks=10,freq = F)
lines(density(WT_data$BW.gram),col="red")

```

Data shape - histogram (3/4)
========================================================

```{r,echo=F,out.width="850px",height="850px"}
par(mfrow=c(2,1))
  hist(WT_data$BW.gram,breaks=10,freq = F,
       xlim=range(WT_data$BW.gram),xlab="Body Weight (gram)")
  lines(density(WT_data$BW.gram),col="red",lwd=3)
  boxplot(WT_data$BW.gram,xlab="Body Weight (gram)",horizontal=T, ylim=range(WT_data$BW.gram))
  abline(v=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
par(mfrow=c(1,1))
```


Data shape - violin plot (4/4)
========================================================
Left: 40%

```{r,echo=F}
par(mfrow=c(2,1))
  hist(WT_data$BW.gram,breaks=10,freq = F,xlab="Body Weight (gram)",
       xlim=range(WT_data$BW.gram))
  lines(density(WT_data$BW.gram),col="red",lwd=3)
  boxplot(WT_data$BW.gram,xlab="Body Weight (gram)",horizontal=T, ylim=range(WT_data$BW.gram))
  abline(v=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
par(mfrow=c(1,1))
```
***


Spread of data - Variance and Standard deviation (1/3)
========================================================

$$
  \begin{aligned}
  \overline x  = \frac{{\displaystyle\sum_{i=1}^n}x_i}n \\
  \\ \\
  \text{Variance} = \sigma^2 = \frac{{\displaystyle\sum_{i=1}^n}(\left|x_i-\overline x\right|)^2}{n-1} \\
  \\ \\
  \text{Standard deviation} = \sigma =\sqrt{\text{Variance}} \\
  \\ \\
  \end{aligned}
$$
```{r,prompt=T}
var.BW.gram<-sum((WT_data$BW.gram-mean.BW.gram)^2)/(nrow(WT_data)-1)
sd.BW.gram<-sqrt(var.BW.gram)
c(var.BW.gram, sd.BW.gram)
```

Spread of data - var() and sd() function (2/3)
========================================================
```{r,prompt=T}
var(WT_data$BW.gram)
sd(WT_data$BW.gram)
```

More about SD and Variance (3/3)
========================================================
- we use the SD more often because it has the same units as the data BUT, if you know one, then you automatically know the other as well.

- in many analysis, variances are used more often, i.e. F-test

Hypothesis testing and ANOVA
========================================================

- SD (standard deviation) and SE (standard error; standard error of sample mean)

- Confidence Interval (CI)

- Hypothesis testing

 -- parametric test:e.g. t-test

 -- non-parametric test: e.g. Wilcoxon test; chi-square test and Fisher's exact test

- Analysis of Variance (ANOVA)


Statistical tests
========================================================

On top of descriptive statistics, R has several statistical tests covering a range of problems and data types.

Some common tests include:
- var.test() - Comparing 2 variances (Fisher's F test)
- t.test() - Comparing 2 sample means with normal errors (Student's t-test)
- wilcox.test() - Comparing 2 means with non-normal errors (Wilcoxon's rank test)
- fisher.test() - Testing for independence of 2 variables in a contingency table (Fisher's exact test)


Hypothesis testing for mean - t-test (1/8)
========================================================

**t.test()**

*one-sample t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,mu=something)
```
*independent t-test*

We are going to discuss this case here.
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=FALSE)
```
*paired t-test*
```{r,warning=FALSE, eval=F}
t.test(Patients_before_treatment,Patients_after_treatment,paired=TRUE)
```

Hypothesis testing for mean - Load data (2/8)
========================================================

Use the WT dataset as example: Is the body weight of WT in Age 8 weeks and Age 16 weeks different. For the purpose of this session, let's assume the mouse body weight is normally distributed in WT 8 weeks and WT 16 weeks.


```{r, warning=FALSE, eval=T,echo=T}
WT_data<-alldata[alldata$Genotype=="+/+",]
boxplot(BW.gram~Age,data=WT_data)
```


Independent t-test example - Calculating variance (3/8)
========================================================
What is the difference in variances between WT Age 8 weeks and WT Age 16 weeks?

F test

$$F= \frac{S^2_x}{S^2_y}
\\
S^2_x:\text{ sample varience for group x}
\\
S^2_y:\text{ sample varience for gorup y}
\\\\
\text{degrees of freedom for the numerator}=n_x-1
\\
\text{degrees of freedom for the denominator}=n_y-1
$$


Calculating F test with R (4/8)
========================================================

We can test for any differences in variances between WT 8 weeks and WT 16 weeks with an F-test using the var.test() function.

$$H_0:\sigma_{WT_8w}^{2}= \sigma_{WT_16w}^{2}
\\
H_a:\sigma_{WT_8w}^{2}\neq \sigma_{WT_16w}^{2}$$

```{r}
WT_8w_data<-WT_data[WT_data$Age=="8" ,]
WT_16w_data<-WT_data[WT_data$Age=="16" ,]
var.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram)

```

R objects (s3 and s4) (5/8)
========================================================
Left:30% The data type holding the result var.test() is a little more complex than the data types we have looked.

In R, special objects (S3 or S4 objects) can be created which have methods associated to them. The result from var.test is an object of class htest.

Since we have not come across this before, in order to discover its structure we can use the str() function with the object of interest as the argument.
```{r}
result <- var.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram)
str(result)
```


R objects (s3 and s4) (6/8)
========================================================
Now we know the structure and class of the htest object we can access the slots containing information we want just as with a named list.

The p-value
```{r}
result$p.value
```
The statistic
```{r}
result$statistic
```
The data used in function call
```{r}
result$data.name
```

Independent t-test (7/8)
========================================================
We have ascertained that Ration1 and Ration2 have similar variances. We can therefore perform a standard t-test to assess the significance of differences between these groups.

$$H_0:\mu_{WT_{8w}}= \mu_{WT_{16w}}
\\
H_a:\mu_{WT_{8w}}\neq \mu_{WT_{16w}}$$

```{r}
test_res <- t.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram,alternative ="two.sided", var.equal = T)
test_res
```

T-test example - Specifying a formula (8/8)
========================================================
The same result to that shown could be achieved by specifying a formula for the comparison. Here we wish to compare 8 weeks versus 16 weeks so we could simply specify the formula and the data to be used.

```{r}
result_formula <- t.test(BW.gram~Age,WT_data,alternative ="two.sided", var.equal = T)
result_formula
```

Non-parametric test
========================================================
  
  Non-parametric statistical hypothesis test is a test that is not based on probability distribution for the dependant variable. 

It doesn't repuire the dependent varible to be normally distributed.

**wilcox.test()**

Wilcoxon Signed-Rank Test is one of the Non-parametric statistical hypothesis tests. It is a good alternative to t-tests without assuming the dependent variables to follow the normal distribution.

t-test and Wilcoxon test alternatives
========================================================

**t.test()**

*one-sample t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,mu=something)
```
*independent t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=FALSE)
```
*paired t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=TRUE)
```
***
**wilcox.test()**

*one-sample Wilcoxon: Signed-Rank Test*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,mu=something)
```
*Wilcoxon Rank Sum Test: Mann-Whitney U*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,groupB,paired=FALSE)
```
*paired Wilcoxon:Signed-Rank Test*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,groupB,paired=TRUE)
```

Wilcoxon test - wilcox.test()
========================================================

Wilcoxon Signed-Rank Test is one of the Non-parametric statistical hypothesis tests. It is a good alternative to t-tests without assuming them to follow the normal distribution.

$$H_0: \text{median}_{a}- \text{median}_{b} = 0
\\
H_a: \text{median}_{a}- \text{median}_{b}\neq 0$$

Back to our mouse dataset
========================================================

Is there body weight different between WT and KO mice?

```{r}
WT_dbdb_data<-alldata[alldata$Genotype!="db/+",]
WT_dbdb_data<-droplevels(WT_dbdb_data)
boxplot(BW.gram~Genotype,data=WT_dbdb_data)
```


Wilcoxon test
========================================================
**qqnorm()** and **qqline()**

Check normal distribution with normal quantile plots for WT data

```{r}
WT4wilcox<-WT_dbdb_data[WT_dbdb_data$Genotype=="+/+",]

qqnorm(WT4wilcox$BW.gram)
qqline(WT4wilcox$BW.gram)
shapiro.test(WT4wilcox$BW.gram)

```

Wilcoxon test
========================================================
**qqnorm()** and **qqline()**

Check normal distribution with normal quantile plots for db/db data

```{r}
KO4wilcox<-WT_dbdb_data[WT_dbdb_data$Genotype=="db/db",]
qqnorm(KO4wilcox$BW.gram)
qqline(KO4wilcox$BW.gram)
shapiro.test(KO4wilcox$BW.gram)
```

Wilcoxon test (Mann-Whitney U)
========================================================
**wilcox.test()**

```{r}

wilcox.test(WT4wilcox$BW.gram,KO4wilcox$BW.gram, paired=F)

```

fisher.test()
========================================================

Given two gene lists, tests the significance of their overlap in comparison with a genomic background.

$$H_0:\text{ the odds ratio is no larger than 1}
\\
H_a:\text{ the odds ratio is larger than 1 }$$

Assuming there are 20,000 genes in the mouse genome, we have gene list A (300 genes) and B (50 genes). The number of overlap genes between list A and B is 5. Is the overlap between the two list significant?

fisher.test()
========================================================
create a contigency table

```{r,prompt=T}
cmatrix<-matrix(c(5,40,295,19960),byrow=T,ncol=2,dimnames=list(c("In.B","Not.In.B"),c("In.A","Not.In.A")))
cmatrix

```

fisher.test()
========================================================
**fisher.test()**

```{r}
fisher.test(cmatrix)
```

  
ANOVA (1/5)
========================================================
  
  Compute analysis of variance (or deviance), a.k.a. ANOVA, for one or more fitted model objects.

ANOVA is a statistical method that uses F-test to test

$$H_0:\mu_{1}= \mu_{2}=... \mu_{k}$$
  
  by comparing the variability between groups to the variability within groups
  
Assume that 

(1) all samples are independent and have >2 categorical groups; 

(2) dependent variable is continuous

(3) data of each group is normally distributed

(4) homogeneity of variances

ANOVA (2/5)
========================================================
```{r}
boxplot(BW.gram~Genotype,data=alldata)
```


ANOVA - use the lm() function (3/5)
========================================================
```{r}
lmPG<-lm(formula = BW.gram ~ Genotype,data = alldata)
lmPG

```

ANOVA - use the anova() function (4/5)
========================================================
```{r}
anova_PG<-anova(lmPG)
anova_PG
```

ANOVA - post-hoc analysis (5/5)
========================================================
**TukeyHSD** - Test which of the groups have different means

```{r}
TukeyHSD(aov(lmPG))
```

Correlation (1/6)
=========================================================

A common task in statistical analysis is to investigate the linear relationship between pairs of numeric vectors.

This can be done by identifying the correlation between numeric vectors using the **cor()** function in R.

In this example we use **cor()** to identify the Pearson correlation between two variables.  The **method** argument may be set to make use of different correlation methods.

- Perfectly posively correlated vectors will return 1
- Perfectly negatively correlated vectors will return -1
- Vectors showing no or little linear correlation will be close to 0.


Correlation between vectors (2/6)
=========================================================

```{r,prompt=T}
x <- rnorm(100,10,2)
z <- rnorm(100,10,2)
y <- x
cor(x,y) #
cor(x,-y)
cor(x,z)
```
***
```{r,echo=F,prompt=T}
par(mfrow=c(3,1))
plot(x,y) #
plot(x,-y)
plot(x,z)

par(mfrow=c(1,1))
```

Correlation example (3/6)
=========================================================

Example of our mouse data. We would like to see whether there is a relationship between body weight and the percentage of fat tissue in the WT mice.

```{r,prompt=T}
KO_data<-alldata[alldata$Genotype=="db/db",]
head(KO_data)
cor(KO_data$BW.gram,KO_data$FatTissue.percent)

```
***
```{r,echo=F,prompt=T,fig.width=6,fig.height=5.5,dpi=300}
library("ggplot2")
p <- ggplot(KO_data, aes(BW.gram, FatTissue.percent))
p + geom_point()+
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"))

```

Correlation over a matrix (4/6)
=========================================================
left: 70%
Often we wish to apply correlation analysis to all columns or rows in a matrix in a pair-wise manner. To do this in R, we can simply pass the **cor()** function a single argument of the numeric matrix of interest. The **cor()** function will then perform all pair-wise correlations between columns.

- subset mouse dataset
```{r,prompt=T}
mouse4cor<-KO_data[,c(6:8)]; 

```

Correlation over a matrix (5/6)
=========================================================
```{r,prompt=T}
cor(mouse4cor)
```
```{r,eval=T,echo=F,fig.width=4,fig.height=2.5,dpi=300}
image(cor(mouse4cor),axes=F)
mtext(colnames(mouse4cor),side=2,at=seq(0,1,length.out=3),las=1,cex=0.6)
mtext(colnames(mouse4cor),side=3,at=seq(0,1,length.out=3),las=2,cex=0.6)

```

Correlation (6/6)
========================================================
```{r,prompt=T,fig.width=5,fig.height=5,dpi=300,out.width="650px",height="650px"}
pairs(mouse4cor)
```
Linear regression (1/23)
=========================================================

We have seen how we can find the linear correlation between two sets of variables using **cor()** function.

R also provides a comprehensive set of tools for regression analysis including the well used linear modeling function **lm()**

- least square method

*minimize the vertical distance between the fitted line and data points* 

```{r,echo=F,prompt=T,fig.width=3,fig.height=3,dpi=300,out.width="650px",height="650px"}
library("ggplot2")

pwh <-  ggplot(KO_data, aes(FatTissue.percent, BW.gram))
pwh + geom_point() + labs(x = "Body weight (gram)",y="Fat Tissue (%)") + 
   stat_smooth(method="lm",se=T)

```


Linear regression (2/23)
=========================================================
left: 70%
We use KO mouse dataset as example and see whether we can use mouse body weight to predict the percentage of fat tissue.
```{r,prompt=T}
KO_data<-alldata[alldata$Genotype=="db/db",]
head(KO_data)

```
***
```{r,echo=F,prompt=T,fig.width=4,fig.height=4,dpi=300,out.width="820px",height="820px"}
pwh <-  ggplot(KO_data, aes(BW.gram, FatTissue.percent))
pwh + geom_point() + labs(x = "Body weight (gram)",y="Fat Tissue (%)")
```


Linear regression (3/23)
=========================================================
The **lm()** function fits a linear regression to your data and provides useful information on the generated fit.

In the example below we fit a linear model using  **lm()** on the *KO_data* dataset with *FatTissue.percent* (Y) as the dependent variable and *BW.gram* (X) as the explanatory variable.
```{r,prompt=T}
lmResult<-lm(formula = FatTissue.percent ~ BW.gram, data = KO_data)
lmResult
```


Interpret output of lm() (4/23)
=========================================================

As we have seen, printing the model result provides the intercept and slope of line.
To get some more information on the model we can use the **summary()** function
```{r,prompt=T}
summary(lmResult)
```

Interpret output of lm() - coefficients (5/23)
=========================================================
left: 70%
```{r,prompt=T}
lmResult$coefficients
```
From the **$coefficients** of object *lmResult*, we know the equation for the best fit is

**$$Y = 36.6140076 + 0.5026053 *X$$**

**$$f(x)  = b_0 + b_1x$$**

$$b_0\text{: the value of f(x) when x =0}$$

```{r}
# the Intercept 36.6140076 is the expected percentage of fat tissue of a 0 body weight
# not interesting to any biological questions
```

$$b_1\text{: the amount of f(x) will change when x changes 1 unit}$$

```{r}
# For every gram increased in the mice body weight, we expect 0.50 (%) increased in the Fat tissue

```
***
```{r,echo=F,prompt=T,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
pwh <-  ggplot(KO_data, aes(BW.gram, FatTissue.percent))
pwh + geom_point() + stat_smooth(method = "lm",se=F) +
  labs(x = "Body weight (gram)",y="Fat Tissue (%)")
```


More about coefficients (6/23)
=========================================================

Predict the percentage of fat tissue with the body weight information.

If we have 3 KO mice with weight = 40, 55 and 66 grams, how do we predict their percentage of fat tissue?


Use the information from the *$coefficients*
```{r,prompt=T}
new_mouse_bw<-c(40,55,66)
beta0<-lmResult$coefficients[1]
beta1<-lmResult$coefficients[2]

predicted_new_fat<-beta0+beta1*new_mouse_bw
predicted_new_fat
```

Or use the *predict()*
```{r}
new_mouse_bw_df <- data.frame(BW.gram=c(40,55,66))
cleaver_predicted_fat<-predict(lmResult,new_mouse_bw_df)
cleaver_predicted_fat

```



Interpret output of lm() - residuals (7/23)
=========================================================

The **residuals** are the difference between the predicted and actual values.
To retrieve the residuals we can access the slot or use the **resid()** function.

```{r,prompt=T,echo=T}
summary(resid(lmResult))
summary(lmResult$residual)
```
Ideally you would want your residuals to be normally distributed around 0.

$$
E[e_{i}]=0
$$

More about residuals (8/23)
=========================================================

Plot the residuals

```{r,echo=T,fig.width=9,fig.height=7,dpi=300,out.width="1000",height="750"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)

```

More about residuals (9/23)
=========================================================

Residual is the vertical distance between the observed data and the regression line. It has the same unit as the dependent variable.

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)
y<-KO_data$FatTissue.percent;x<-KO_data$BW.gram
yhat<-predict(lmResult)
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),
        c(y[i],yhat[i]),
        col="red",lwd=2)
}
```

More about residuals (10/23)
=========================================================

SSE shows the residual variability

It shows the variability that cannot be explained by the regression model

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)
y<-KO_data$FatTissue.percent;x<-KO_data$BW.gram
yhat<-predict(lmResult)
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),
        c(y[i],yhat[i]),
        col="red",lwd=2)
}
```
***
$$
Error_i = y_i - \hat{y_i}
\\
y_i\text{: the observed weight of ith kid}
\\
\hat{y_i}\text{: the predicted weight of ith kid}
\\
Error_i^2  = (y_i - \hat{y_i})^2
\\
\text{- sum of the square of the residuals (SSE)}
\\
SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2
$$



More about residuals (11/23)
=========================================================

Plot the residuals against the independent variable (X), i.e. the body weight It makes the residual accessment easiler by eyes.

```{r,echo=T,fig.width=9,fig.height=7,dpi=300,out.width="1000px",height="750px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)

```

More about residuals (12/23)
=========================================================

Plot the residuals against the independent variable (X), i.e. the body weight. 

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}
```

More about residuals (13/23)
=========================================================

Plot the residuals against the independent variable (X)

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}

```

***
$$
Error_i = y_i - \hat{y_i}
\\

Error_i^2  = (y_i - \hat{y_i})^2
\\
\text{- sum of the square of the residuals (SSE)}
\\
SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2
$$


Interpret output of lm() - R-squared (14/23)
=========================================================

- The **R-squared** value represents the proportion of variability in the response variable that is explained by the explanatory variable.

- A high **R-squared** here indicates that the line fits closely to the data.

```{r,prompt=T,echo=T}
summary(lmResult)$r.squared
```


More about R-squared (15/23)
=========================================================

- Question: How would you describe (or summarize) the percentage of fat tissue when the **body weight information is absence**? Which information you would use to predict a new mouse's percentage of fat tissue?

```{r,prompt=T,echo=T}
KO_data$FatTissue.percent
```

More about R-squared (16/23)
=========================================================

- Question: How would you describe (or summarize) the percentage of fat tissue when the **body weight information is absence**? Which information you would use to predict a new mouse's percentage of fat tissue?

- mean might be a good choice

```{r,prompt=T,echo=T}
mean(KO_data$FatTissue.percent)
```

- If we have a new child, we could assume that the kid's weight is around 38.384 pounds.

More about R-squared (17/23)
=========================================================

- Question: How would you describe (or summarize) kid's weight when the **height information is absence**? Which information you would use to predict a new child's weight?

- mean might be a good choice

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(-30,150),
     ylab="weight (pounds)", xlab="x")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)
```

More about R-squared - TSS (18/23)
=========================================================

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(0,100),
     ylab="fat tissue (%)", xlab="x")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)
segments(x0=c(1:nrow(KO_data)),y0=KO_data$FatTissue.percent,
         x1=c(1:nrow(KO_data)),y1=mean(KO_data$FatTissue.percent),col="red",lwd=2)
```
***
Residuals from the mean: assuming the independent variable (X), i.e. height in our case, does not exist

$$
TSS=\text{Total Sum of Squares}=\sum_{i=1}^n(y_i-\overline y)^2
$$


More about  about R-squared (19/23)
=========================================================

Residuals from the mean: assuming the independent variable (X), i.e. height in our case, does not exist

```{r,echo=F}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(-30,150),
     ylab="rediduals (fat tissue %)", xlab="body weight (gram)")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)

segments(x0=c(1:nrow(KO_data)),y0=KO_data$FatTissue.percent,
         x1=c(1:nrow(KO_data)),y1=mean(KO_data$FatTissue.percent),col="red",lwd=2)
```

- Total Sum of Squares (TSS)

$$
  \begin{aligned}
  TSS  = \sum_{i=1}^{n}(y_i-\overline y)^2
  \end{aligned}
$$

***

Residuals from the model

```{r,echo=F}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-30,150),
     ylab="rediduals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}
```
- Sum of the square of the residuals (SSE)
$$
SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2
$$

More about R-squared (20/23)
=========================================================

```{r,warning=F,echo=F,fig.width=6,fig.height=6,dpi=300,out.width="720px",height="720px"}

library(ggplot2,quietly=T)
e<-c(resid(lm(FatTissue.percent~1,data=KO_data)),
     resid(lm(FatTissue.percent~BW.gram,data=KO_data)))
fit4figure<-factor(c(rep("Fit2mean",nrow(KO_data)),
                   rep("Fit2BW",nrow(KO_data))), levels=c("Fit2mean","Fit2BW"))
ggplot(data.frame(e=e,fit=fit4figure),aes(y=e,x=fit,fill=fit)) + 
  geom_dotplot(binaxis="y", stackdir="center",binwidth = 1) + xlab("Model fit") +
  ylab("residuals (fat tissue %)")

```

More about R-squared - Calculating R-squared (21/23)
=========================================================

The fraction of variability in the independent variable (Y; or the *weight* in this example) that can be explained by the explanatory variable (X; or the *height* in this example).

$$
TSS=\text{Total Sum of Squares}=\sum_{i=1}^n(y_i-\overline y)^2
\\
SSE=\text{Sum of the Square of the residuals}=\sum_{i=1}^n(y_i-\hat{y})^2
$$

```{r,prompt=T}
SSE<-sum(resid(lm(FatTissue.percent~BW.gram,data=KO_data))^2)
TSS<-sum(resid(lm(FatTissue.percent~1,data=KO_data))^2)
R_square<-1-(SSE/TSS)
R_square
summary(lmResult)$r.squared
```


Interpret output of lm() - F-statistics (22/23)
=========================================================

The R-squared shows the fraction of the total variability that is explained by the linear relationship with the explanatory variable. However, it does not provide a formal hypothesis test for this relationship. 

The F-test results from linear models also provides a measure of significance for a variable not being relevant

```{r,prompt=T,echo=T}
summary(lmResult)$fstatistic
```

More about F-statistics - Calculating F-stat (23/23)
=========================================================

$$
F=\frac{MSM}{MSE}=\frac{\text{mean of the explained variance}}{\text{mean of the unexplained variance}}=\frac{({\displaystyle\frac{SSM}1})}{({\displaystyle\frac{SSE}{n-2}})}
$$

```{r,prompt=T}

n=nrow(KO_data)
SSM <- sum((predict(lmResult) - mean(KO_data$FatTissue.percent))^2)
MSE <-sum(lmResult$residuals^2)/(n-2)

MSM <-SSM/1

MSM/MSE

summary(lmResult)$fstatistic
```